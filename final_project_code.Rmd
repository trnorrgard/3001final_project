---
title: "final_project"
author: "Teagan Norrgard, trn8cwf"
date: "11/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, include=FALSE, echo=FALSE}
library(dplyr)
library(tidyverse)
library(e1071)
library(ggplot2)
library(plotly)
library(htmltools)
library(devtools)
library(NbClust)
```


### use KNN to predict rating (threshold = 95?)

based on these features, a renter should be able to predict if they will enjoy their stay at the property or not

- ratings depending on price, room type, neighborhood, superhost, property_type, accomodates

refactor property type


### First we read in our dataset.

Our dataset was obtained from Kaggle but the data itself was sourced from Inside Airbnb
The first thing we did was read in our data.
```{r}
boston <- read.csv("Boston_Airbnb_copy.csv")
```


### Next we got to cleaning our data. 
This included sub-setting columns we thought would make for interesting predictions, renaming columns so that they would be easier to call, and making 'price' and 'cleaning_fee' numeric variables.
```{r, warning=FALSE}
boston <- boston%>%
  select(c("name","neighbourhood_cleansed","latitude", "longitude", "room_type", 
           "accommodates", "price", "review_scores_rating", 
           "host_is_superhost", "property_type"))

boston <-  boston%>%
  rename(superhost = host_is_superhost, neighborhood = neighbourhood_cleansed)

boston$price = as.numeric(gsub("\\$", "", boston$price))
```


### Then we made some basic exploration tables and visualizations.

Here we looked at neighborhood counts, and saw that the most popular neighborhoods are Jamaica Plain, South End, Back Bay, Fenway, Dorchester, and Allston.

We thought grouping by neighborhood would be an easy way to do some basic exploration
```{r}
table(boston$neighborhood)
```


Here we made a table of the median values the numeric columns compared by neighborhood
```{r}
## could be interesting to compare to means
bost_med_table <- boston%>%
  group_by(neighborhood)%>%
  summarise(medianReview = median(review_scores_rating, na.rm=T),
            medianPrice = median(price, na.rm=T))

bost_med_table
```


And made a bar graph comparing the median prices by neighborhood.
```{r}
plot <- ggplot(data=bost_med_table, aes(x=neighborhood, y=medianPrice, fill=medianPrice))+
  scale_fill_gradient(low = "dark red", high = "cornflowerblue")+
  geom_bar(stat='identity')+
  theme(axis.text.x = element_text(angle=90))+
  labs(x="Neighborhood", y="Median Price of Airbnb per Night", title="Distribution of Airbnb prices per night over Neighborhoods in Boston")


plot
```


We made a table of the number of room types by neighborhood.
It is important here to keep in mind the table of neighborhood counts
```{r}
boston%>%
  group_by(neighborhood)%>%
  select(room_type)%>%
  table()
```


And we see here that the top 100 most expensive Airbnbs are pretty evenly distributed over the neighborhoods. Back Bay has the highest number with 17 but doesn't seem so high that it is an outlier. This also makes sense because Back Bay was one of the most popular neighborhoods in the first place
```{r}
boston_top100 <- boston%>%
  arrange(desc(price))

## we see here that the top 100 most expensive places are pretty evenly distributed over neighborhoods,
## Back Bay has the highest number with 17 but doesn't seem so high that it is an outlier.
## this also makes sense because Back Bay was one of the most popular neighborhoods in the first place
head(boston_top100, 100)%>%
  select(neighborhood)%>%
  table()

```




























## Clustering

seeing if this makes a difference
```{r}
table(boston$room_type)
boston$room_type <- fct_collapse(boston$room_type,
                                  v1 = "Entire home/apt",
                                  v2 = "Private room",
                                  v3 = "Shared room")

boston$room_type = as.numeric(gsub("v", "", boston$room_type))
```


First we picked which variables we thought would do the best at predicting neighborhoods.
We chose to use price and rating.
```{r}
clust_boston <- boston[, c("price", "review_scores_rating", "room_type")]
```


There are a few NAs in the price and rating columns, so we replaced them with the median
```{r}
## a better thing to do would be replace with the median according to its neighborhood, but idk how to do that
sum(is.na(clust_boston$review_scores_rating))
sum(is.na(clust_boston$price))

clust_boston$review_scores_rating[is.na(clust_boston$review_scores_rating)] <- median(clust_boston$review_scores_rating, na.rm=T)
clust_boston$price[is.na(clust_boston$price)] <- median(clust_boston$price, na.rm=T)

sum(is.na(clust_boston$review_scores_rating))
sum(is.na(clust_boston$price))

## then normalize the data
normalize <- function(x){
 (x - min(x)) / (max(x) - min(x))
}

clust_boston[1:2] <- lapply(clust_boston[1:2], normalize)

```

Then we sorted the neighborhoods into groups of neighborhoods that were close in proximity
##### ignore this one
```{r}
boston$neighborhood_groups <- fct_collapse(boston$neighborhood,
                                           Suburbs = c("Jamaica Plain", "Roslindale", 
                                                       "Dorchester","Roxbury", 
                                                       "West Roxbury", "Hyde Park", 
                                                       "Mattapan"),
                                           South_Boston = c("South Boston", 
                                                         "South Boston Waterfront"),
                                           Cambridge = c("Brighton", "Allston"),
                                           East_Boston = c("Charlestown", "East Boston"),
                                           Fenway = c("Fenway", "Mission Hill", 
                                                      "Longwood Medical Area"),
                                           Central_Boston = c("Bay Village", "Back Bay",
                                                              "Beacon Hill", "West End",
                                                              "North End", "Downtown",
                                                              "South End", "Chinatown",
                                                              "Leather District"))

```

## run this one
```{r}
boston$neighborhood_groups <- fct_collapse(boston$neighborhood,
                                           Outskirts = c("Jamaica Plain", "Roslindale", 
                                                       "Dorchester","Roxbury", 
                                                       "West Roxbury", "Hyde Park", 
                                                       "Mattapan", "Brighton", "Allston"),
                                           Central_Boston = c("Bay Village", "Back Bay",
                                                              "Beacon Hill", "West End",
                                                              "North End", "Downtown",
                                                              "South End", "Chinatown",
                                                              "Leather District", "Fenway", 
                                                              "Mission Hill", "Longwood Medical Area", 
                                                              "South Boston", "South Boston Waterfront", 
                                                              "Charlestown", "East Boston"))
```


Then we used the elbow method to figure out how many clusters to choose.
```{r}
explained_variance = function(data_in, k){
  set.seed(1)
  kmeans_obj = kmeans(data_in, centers = k, algorithm = "Lloyd", iter.max = 30)
  var_exp = kmeans_obj$betweenss / kmeans_obj$totss
  var_exp  
}



explained_var_boston = sapply(1:10, explained_variance, data_in = clust_boston)
explained_var_boston

elbow_boston = data.frame(k = 1:10, explained_var_boston)
ggplot(elbow_boston, 
       aes(x = k,  
           y = explained_var_boston)) + 
  geom_point(size = 4) +
  geom_line(size = 1) + 
  xlab('k') + 
  ylab('Inter-cluster Variance / Total Variance') + 
  theme_light()
```
Based on this graph, it looks like 2 clusters will give us the best model without over-fitting.

So we ran the clustering algorithm with 2 clusters
```{r, warning=FALSE}
set.seed(123)
kmeans_obj_boston = kmeans(clust_boston, centers = 2, 
                        algorithm = "Lloyd")

kmeans_obj_boston

clusters_boston = as.factor(kmeans_obj_boston$cluster)
```

```{r}
num_boston = kmeans_obj_boston$betweenss

# Total variance, "totss" is the sum of the distances
# between all the points in the data set.
denom_boston = kmeans_obj_boston$totss

# Variance accounted for by clusters.
(var_exp_boston = num_boston / denom_boston)
```


And finally visualize the results
```{r}

## would this be  better if we manually grouped neighborhoods by location (factor collapse?)
## and then there wouldnt be 25 different things to look at

## also i will try to make a plotly graph later so that you can do the hover text

neighborhood_clusters = as.factor(kmeans_obj_boston$cluster)

ggplot(boston, aes(x = price, 
                            y = review_scores_rating,
                            color = neighborhood_groups,
                            shape = neighborhood_clusters)) + 
  geom_point(size = 2) +
  ggtitle("Price vs Rating of Boston Airbnbs") +
  xlab("Price per Night") +
  ylab("Review Score (out of 100)") +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2"),
                     values = c("1", "2")) +
  theme_light()
```

```{r, warning=FALSE, include=FALSE, echo=FALSE}
library(lubridate)
library(data.table)
library(ggrepel)
library(ggmap)
```

Appending cluster groups to boston dataset
```{r}
boston$clusters <- neighborhood_clusters
```


To make a map visualization, I needed to install packages with a special key
```{r}
if(!requireNamespace("devtools")) install.packages("devtools")
devtools::install_github("dkahle/ggmap", ref = "tidyup", force=TRUE)

library('ggmap')
 
ggmap::register_google(key = 'AIzaSyBYxfE_HmtWRQ-YgKd4I7-QZ-fI0AzP4zQ')
```


I set the center coordinates as the center of Boston, and adjusted to fit the Airbnb locations
```{r}
p <- ggmap(get_googlemap(center = c(lon = -71.0759, lat = 42.3401),
                    zoom = 13, scale = 2,
                    maptype ='terrain',
                    color = 'color'))+ 
  geom_point(aes(x = longitude, y = latitude,  colour = clusters), data = boston, size = 0.5) + 
  theme(legend.position="bottom")
p
```
lol is this showing that it is predicting terribly
theres no clear grouping of clusters, everything is mixed



## confusion based on clusters
```{r}
clust_boston$neighborhood_groups <- boston$neighborhood_groups
clust_boston$clusters <- neighborhood_clusters

# change neighborhood group and cluster assignment to factors
clust_boston[,c(4,5)] <- lapply(clust_boston[,c(4,5)], as.factor)
```

train, tune, test partitions, assign features
```{r}
train_index <- createDataPartition(clust_boston$neighborhood_groups,
                                           p = .7,
                                           list = FALSE,
                                           times = 1)
train <- clust_boston[train_index,]
tune_and_test <- clust_boston[-train_index, ]


tune_and_test_index <- createDataPartition(tune_and_test$neighborhood_groups,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

features <- as.data.frame(train[,-c(4)])
target <- train$neighborhood_groups
```

```{r}
set.seed(123)
boston_dt <- train(x=features,
                    y=target,
                    method="rpart")
 
boston_dt
varImp(boston_dt)

# Let's predict and see how we did. 
dt_predict_1 = predict(boston_dt,tune,type= "raw")

confusionMatrix(as.factor(dt_predict_1), 
                as.factor(tune$neighborhood_groups), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")
```




## Text Mining
 could we do text mining on the amenities column, 
 find the words w the best value, 
 one hot encode for including those words, 
 predict price on whether certain amenities are offered ??? 
 
 that all seems really complicated lol
```{r}

```







