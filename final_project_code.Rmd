---
title: "final_project"
author: "Teagan Norrgard, trn8cwf"
date: "11/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE, include=FALSE, echo=FALSE}
library(dplyr)
library(tidyverse)
library(e1071)
library(ggplot2)
library(plotly)
library(htmltools)
library(devtools)
library(NbClust)
```


###First we read in our dataset.

Our dataset was obtained from Kaggle but the data itself was sourced from Inside Airbnb
The first thing we did was read in our data.
```{r}
boston <- read.csv("/Users/teagannorrgard/3001final_project/Boston_Airbnb_copy.csv")
```


### Next we got to cleaning our data. 
This included sub-setting columns we thought would make for interesting predictions, renaming columns so that they would be easier to call, and making 'price' and 'cleaning_fee' numeric variables.
```{r}
boston <- boston%>%
  select(c("name","neighbourhood_cleansed","latitude", "longitude", "room_type", 
           "accommodates", "price", "cleaning_fee", "minimum_nights", "number_of_reviews",
           "review_scores_rating","host_response_rate", "host_acceptance_rate", 
           "host_is_superhost", "amenities"))

boston <-  boston%>%
  rename(superhost = host_is_superhost, neighborhood = neighbourhood_cleansed)

boston$price = as.numeric(gsub("\\$", "", boston$price))
boston$cleaning_fee = as.numeric(gsub("\\$", "", boston$cleaning_fee))
```


### Then we made some basic exploration tables and visualizations.

Here we looked at neighborhood counts, and saw that the most popular neighborhoods are Jamaica Plain, South End, Back Bay, Fenway, Dorchester, and Allston.
```{r}
table(boston$neighborhood)
```


Here we made a table of the median values the numeric columns compared by neighborhood
```{r}
## could be interesting to compare to means
bost_med_table <- boston%>%
  group_by(neighborhood)%>%
  summarise(medianReview = median(review_scores_rating, na.rm=T), medianNumReviews = median(number_of_reviews, na.rm=T),
            medianPrice = median(price, na.rm=T), medianCleanFee = median(cleaning_fee, na.rm=T))

bost_med_table
```


And made a bar graph comparing the median prices by neighborhood.
```{r}
plot <- ggplot(data=bost_med_table, aes(x=neighborhood, y=medianPrice, fill=medianPrice))+
  scale_fill_gradient(low = "dark red", high = "cornflowerblue")+
  geom_bar(stat='identity')+
  theme(axis.text.x = element_text(angle=90))+
  labs(x="Neighborhood", y="Median Price of Airbnb per Night", title="Distribution of Airbnb prices per night over Neighborhoods in Boston")


plot
```


We made a table of the number of room types by neighborhood.
It is important here to keep in mind the table of neighborhood counts
```{r}
boston%>%
  group_by(neighborhood)%>%
  select(room_type)%>%
  table()
```


And we see here that the top 100 most expensive Airbnbs are pretty evenly distributed over the neighborhoods. Back Bay has the highest number with 17 but doesn't seem so high that it is an outlier. This also makes sense because Back Bay was one of the most popular neighborhoods in the first place
```{r}
boston_top100 <- boston%>%
  arrange(desc(price))

## we see here that the top 100 most expensive places are pretty evenly distributed over neighborhoods,
## Back Bay has the highest number with 17 but doesn't seem so high that it is an outlier.
## this also makes sense because Back Bay was one of the most popular neighborhoods in the first place
head(boston_top100, 100)%>%
  select(neighborhood)%>%
  table()

```


price, review_scores_rating one more?? to predict neighborhoods

is_host_superhost

use text analysis to predict rating (amenities, summary, description, space)
use clustering to predict neighborhood


## Clustering

First we picked which variables we thought would do the best at predicting neighborhoods.
We chose to use price and rating.
```{r}
clust_boston <- boston[, c("price", "review_scores_rating")]
```

There are a few NAs in the price and rating columns, so we replaced them with the median
```{r}
## a better thing to do would be replace with the median according to its neighborhood, but idk how to do that
sum(is.na(clust_boston$review_scores_rating))
sum(is.na(clust_boston$price))

clust_boston$review_scores_rating[is.na(clust_boston$review_scores_rating)] <- median(clust_boston$review_scores_rating, na.rm=T)
clust_boston$price[is.na(clust_boston$price)] <- median(clust_boston$price, na.rm=T)

sum(is.na(clust_boston$review_scores_rating))
sum(is.na(clust_boston$price))

```


Then we used the elbow method to figure out how many clusters to choose.
```{r}
explained_variance = function(data_in, k){
  set.seed(1)
  kmeans_obj = kmeans(data_in, centers = k, algorithm = "Lloyd", iter.max = 30)
  var_exp = kmeans_obj$betweenss / kmeans_obj$totss
  var_exp  
}



explained_var_boston = sapply(1:10, explained_variance, data_in = clust_boston)
explained_var_boston

elbow_boston = data.frame(k = 1:10, explained_var_boston)
ggplot(elbow_boston, 
       aes(x = k,  
           y = explained_var_boston)) + 
  geom_point(size = 4) +
  geom_line(size = 1) + 
  xlab('k') + 
  ylab('Inter-cluster Variance / Total Variance') + 
  theme_light()
```
Based on this graph, it looks like 4 clusters will give us the best model without overfitting.


Then we ran the clustering algorithm with 4 clusters
```{r}
set.seed(123)
kmeans_obj_boston = kmeans(clust_boston, centers = 4, 
                        algorithm = "Lloyd")

clusters_boston = as.factor(kmeans_obj_boston$cluster)
```


And finally visualize the results
```{r}

## would this be  better if we manually grouped neighborhoods by location (factor collapse?)
## and then there wouldnt be 25 different things to look at

## also i will try to make a plotly graph later so that you can do the hover text

neighborhood_clusters = as.factor(kmeans_obj_boston$cluster)

ggplot(boston, aes(x = price, 
                            y = review_scores_rating,
                            color = neighborhood,
                            shape = neighborhood_clusters)) + 
  geom_point(size = 2) +
  ggtitle("Price vs Rating of Boston Airbnbs") +
  xlab("Price per Night") +
  ylab("Review Score (out of 100)") +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4"),
                     values = c("1", "2", "3", "4")) +
  theme_light()
```

## if possible i  think it would be interesting to use the long/lat columns to put the clusters on a map, that would be easier to visualize than the colors for each neighborhood.

## Text Mining

```{r}

```







