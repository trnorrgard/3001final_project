---
title: "AIRBNB IN BOSTON"
author: "Teagan Norrgard, Akhil Havaldar, Ronica Peraka"
output:
  html_document:
    toc: TRUE
    theme: cerulean
    toc_float: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, include=FALSE, echo=FALSE}
library(dplyr)
library(tidyverse)
library(e1071)
library(ggplot2)
library(plotly)
library(htmltools)
library(devtools)
library(NbClust)
library(caret)
```

### Our Question 

With our data, we wanted to examine the relationship between the price of Airbnb properties, and the neighborhood it is listed in. More specifically, we wanted to see if our model could predict a neighborhood based off of price, and a few other variables we chose later. We thought this would be an interesting dataset to explore with the growth of the tourism industry as vaccination rates rise and cities start to open up. Through our exploration and modeling, we found the areas with highest median price and grouped by room type. 

We also wanted a way to predict the rating of the Airbnb properties. We thought this would be a good addition to our project, since the deal one would get booking an Airbnb does not matter a lot if the experience is bad. We want to find a way to find a good deal with a high rating. We used kNN and evaluated with confusion matrix, logloss, F1, and ROC. 

Our dataset was obtained from Kaggle but the data itself was sourced from Inside Airbnb.

price was used 100% of the time, which is good since our main goal was to examine the relationship between price and neighborhood.


### Cleaning our Data

The first thing we did was read in our data.
```{r}
boston <- read.csv("Boston_Airbnb_copy.csv")
```


Next we sub-setted columns we thought would make for interesting predictions, renamed columns so that they would be easier to call, and made 'price' and 'cleaning_fee' numeric variables.
```{r, warning=FALSE}
boston <- boston%>%
  select(c("name","neighbourhood_cleansed","latitude", "longitude", "room_type", 
           "accommodates", "price", "review_scores_rating", 
           "host_is_superhost", "property_type"))

boston <-  boston%>%
  rename(superhost = host_is_superhost, neighborhood = neighbourhood_cleansed)

boston$price = as.numeric(gsub("\\$", "", boston$price))
```


### Data Exploration{.tabset}

Here we looked at neighborhood counts, and saw that the most popular neighborhoods are Jamaica Plain, South End, Back Bay, Fenway, Dorchester, and Allston.

We thought grouping by neighborhood would be an easy way to do some basic exploration
```{r}
table(boston$neighborhood)
```


#### Median Values Grouped by Neighborhood
```{r}
## could be interesting to compare to means
bost_med_table <- boston%>%
  group_by(neighborhood)%>%
  summarise(medianReview = median(review_scores_rating, na.rm=T),
            medianPrice = median(price, na.rm=T))

bost_med_table
```


#### Median Prices by Neighborhood
```{r}
plot <- ggplot(data=bost_med_table, aes(x=neighborhood, y=medianPrice, fill=medianPrice))+
  scale_fill_gradient(low = "dark red", high = "cornflowerblue")+
  geom_bar(stat='identity')+
  theme(axis.text.x = element_text(angle=90))+
  labs(x="Neighborhood", y="Median Price of Airbnb per Night", title="Distribution of Airbnb prices per night over Neighborhoods in Boston")


plot
```


#### Room Type by Neighborhood
```{r}
boston%>%
  group_by(neighborhood)%>%
  select(room_type)%>%
  table()
```


#### Top 100 Most Expensive, by Neighborhood
We see here that the top 100 most expensive Airbnbs are pretty evenly distributed over the neighborhoods. Back Bay has the highest number with 17 but doesn't seem so high that it is an outlier. This also makes sense because Back Bay was one of the most popular neighborhoods in the first place.
```{r}
boston_top100 <- boston%>%
  arrange(desc(price))

head(boston_top100, 100)%>%
  select(neighborhood)%>%
  table()

```


### Clustering

First we picked which variables we thought would do the best at predicting neighborhoods.
We chose to use price and rating, and room type since that is where we saw the greatest variation by neighborhood in our exploration.

```{r}
clust_boston <- boston[, c("price", "review_scores_rating", "room_type")]
```

Formatting room_type to be usable in clustering
```{r, warning=FALSE}
table(clust_boston$room_type)
clust_boston$room_type <- fct_collapse(clust_boston$room_type,
                                  v1 = "Entire home/apt",
                                  v2 = "Private room",
                                  v3 = "Shared room")

clust_boston$room_type = as.numeric(gsub("v", "", clust_boston$room_type))
```


There are a few NAs in the price and rating columns, so we replaced them with the median
```{r}
clust_boston$review_scores_rating[is.na(clust_boston$review_scores_rating)] <- median(clust_boston$review_scores_rating, na.rm=T)
clust_boston$price[is.na(clust_boston$price)] <- median(clust_boston$price, na.rm=T)

sum(is.na(clust_boston$review_scores_rating))
sum(is.na(clust_boston$price))
```


And then scaled our numeric variables
```{r}
normalize <- function(x){
 (x - min(x)) / (max(x) - min(x))
}

clust_boston[1:2] <- lapply(clust_boston[1:2], normalize)

```


Then we sorted the neighborhoods into 2 groups, central Boston and Boston suburbs.
```{r}
boston$neighborhood_groups <- fct_collapse(boston$neighborhood,
                                           Suburbs = c("Jamaica Plain", "Roslindale", 
                                                       "Dorchester","Roxbury", 
                                                       "West Roxbury", "Hyde Park", 
                                                       "Mattapan", "Brighton", "Allston"),
                                           Central_Boston = c("Bay Village", "Back Bay",
                                                              "Beacon Hill", "West End",
                                                              "North End", "Downtown",
                                                              "South End", "Chinatown",
                                                              "Leather District", "Fenway", 
                                                              "Mission Hill", "Longwood Medical Area", 
                                                              "South Boston", "South Boston Waterfront", 
                                                              "Charlestown", "East Boston"))
```


And we used the elbow method to figure out how many clusters to choose.

Based on this graph, it looks like 2 clusters will give us the best model without over-fitting.
This also makes sense because we collapsed the neighborhoods into 2 groups.
```{r, warning=FALSE}
explained_variance = function(data_in, k){
  set.seed(1)
  kmeans_obj = kmeans(data_in, centers = k, algorithm = "Lloyd", iter.max = 30)
  var_exp = kmeans_obj$betweenss / kmeans_obj$totss
  var_exp  
}

explained_var_boston = sapply(1:10, explained_variance, data_in = clust_boston)
explained_var_boston

elbow_boston = data.frame(k = 1:10, explained_var_boston)
ggplot(elbow_boston, 
       aes(x = k,  
           y = explained_var_boston)) + 
  geom_point(size = 4) +
  geom_line(size = 1) + 
  xlab('k') + 
  ylab('Inter-cluster Variance / Total Variance') + 
  theme_light()
```


So we ran the clustering algorithm with 2 clusters
```{r, warning=FALSE}
set.seed(123)
kmeans_obj_boston = kmeans(clust_boston, centers = 2, 
                        algorithm = "Lloyd")

kmeans_obj_boston

clusters_boston = as.factor(kmeans_obj_boston$cluster)
```


### Visualizations{.tabset}

#### Cluster Scatterplot
```{r, warning=FALSE}
neighborhood_clusters = as.factor(kmeans_obj_boston$cluster)

ggplot(boston, aes(x = price, 
                            y = review_scores_rating,
                            color = neighborhood_groups,
                            shape = neighborhood_clusters)) + 
  geom_point(size = 2) +
  ggtitle("Price vs Rating of Boston Airbnbs") +
  xlab("Price per Night") +
  ylab("Review Score (out of 100)") +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2"),
                     values = c("1", "2")) +
  theme_light()
```

```{r, warning=FALSE, include=FALSE, echo=FALSE}
library(lubridate)
library(data.table)
library(ggrepel)
library(ggmap)
```

#### Map of Airbnbs and Their Clusters

We thought a map where you could see the physical location of the properties would be the best way to visualize our model, so we appended the clusters to our dataset and installed the required packages.
```{r}
boston$clusters <- neighborhood_clusters
```

```{r, incluse=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
if(!requireNamespace("devtools")) install.packages("devtools")
devtools::install_github("dkahle/ggmap", ref = "tidyup", force=TRUE)

library('ggmap')
 
ggmap::register_google(key = 'AIzaSyBYxfE_HmtWRQ-YgKd4I7-QZ-fI0AzP4zQ')
```

Since this map is not interactive, we adjusted the center of the map to fit all of the properties and zoomed as far as we could without Airbnbs  getting cut off. Here, of the 3585 observations, only 5 did not fit in the map.
```{r, message=FALSE}
map1 <- ggmap(get_googlemap(center = c(lon = -71.0759, lat = 42.319),
                    zoom = 12, scale = 2,
                    maptype ='terrain',
                    color = 'color'))+ 
  geom_point(aes(x = longitude, y = latitude,  colour = clusters), data = boston, size = 0.5) + 
  theme(legend.position="bottom")
map1
```

#### Zoomed Map

We ran the map again, this time zooming in on central Boston to get a clearer look at it.
Only 1747 properties are shown in this map, but we see here that our model is doing a pretty good job at predicting Airbnbs in Central Boston.
```{r, message=FALSE}
map_zoomed <- ggmap(get_googlemap(center = c(lon = -71.0759, lat = 42.35101),
                    zoom = 14, scale = 2,
                    maptype ='terrain',
                    color = 'color'))+ 
  geom_point(aes(x = longitude, y = latitude,  colour = clusters), data = boston, size = 0.5) + 
  theme(legend.position="bottom")
map_zoomed
```




### Counfustion Matrix

Next we wanted to get a confusion matrix based on clusters to check our accuracy.

We started by making the neighborhood groups and cluster assignment factors
```{r}
clust_boston$neighborhood_groups <- boston$neighborhood_groups
clust_boston$clusters <- neighborhood_clusters

clust_boston[,c(4,5)] <- lapply(clust_boston[,c(4,5)], as.factor)
```


And then partitioned into train, tune, test sets, and assigned our features/target
```{r}
train_index <- createDataPartition(clust_boston$neighborhood_groups,
                                           p = .7,   #the train set is 70% of the data
                                           list = FALSE,
                                           times = 1)
train <- clust_boston[train_index,]
tune_and_test <- clust_boston[-train_index, ]


tune_and_test_index <- createDataPartition(tune_and_test$neighborhood_groups,
                                           p = .5,     #the tune and test sets will each be 15%
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

features <- as.data.frame(train[,-c(4)])
target <- train$neighborhood_groups
```


And finally we ran the model to get our confusion matrix, and checked the variable importance.
```{r}
set.seed(123)
boston_dt <- train(x=features,
                    y=target,
                    method="rpart")
 
varImp(boston_dt)

dt_predict_1 = predict(boston_dt,tune,type= "raw")

confusionMatrix(as.factor(dt_predict_1), 
                as.factor(tune$neighborhood_groups), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")
```


### Conclusions

  Our accuracy is 72% which isn't the best, but we think our model did a good job predicting the neighborhood with the information it was given. We think that if we could have included more information that was in our original dataset, such as amenities, parking availability, and transit information, our model could have likely been more accurate, and we could have even split up our neighborhood groups more. (Maybe groups of downtown, central, suburbs). Unfortunately these variables were in sentence format written by the host, and were difficult to sort through and use in our clustering model.

  The variable importance output shows us that price was by far the most important variable for predicting neighborhood, which is the relationship we wanted to explore in the beginning. Using our map, we think our model is a valuable tool for finding Airbnbs that could be considered a good deal. Since price was used 100% of the time to predict our clusters, we can draw conclustions about the relative price of an Airbnb compared to the neighborhood it is in. Any Airbnb which was actually located in Central Boston but predicted as the Suburbs likely have a price much lower than other similar Airbnbs in Central Boston. The opposite is also true, where incorrectly classifies properties in the Suburbs are likely more expensive than others.







